{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬去创可贴海报：https://www.chuangkit.com/\n",
    "进入每个海报子页面，进行图片和标签爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# from selenium.webdriver.support.wait import WebDriverWait\n",
    "import requests\n",
    "import time\n",
    "# browser_main = webdriver.Chrome('./chromedriver')\n",
    "browser = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "\n",
    "# browser.set_page_load_timeout(100)\n",
    "# browser.set_script_timeout(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 4187\n",
    "with open('./poster/data.txt','a') as f:\n",
    "    for i in range(1,38):\n",
    "        browser_main.get('https://www.chuangkit.com/sj-pi12-or0-pt0-pn{}.html'.format(str(i)))\n",
    "        elements = browser_main.find_elements_by_class_name('useTemplate')\n",
    "        for element in elements:\n",
    "            new_path = element.get_attribute('href')\n",
    "            print('new_path',new_path)\n",
    "            try:\n",
    "                browser.get(new_path)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            browser_pic = browser.find_element_by_class_name('left-thumbnail')\n",
    "            pic_path = browser_pic.find_element_by_tag_name('img').get_attribute('src')\n",
    "            \n",
    "            browser_tags = browser.find_element_by_class_name('right-detail')\n",
    "            tags = browser_tags.find_elements_by_class_name('template-tags-item')\n",
    "#             time.sleep(20)\n",
    "            if pic_path is None:\n",
    "                continue\n",
    "            \n",
    "#             print('pic_path:',pic_path)\n",
    "            r = requests.get(pic_path)\n",
    "            with open('./poster/{}.png'.format(str(count)), 'wb') as ff:\n",
    "                ff.write(r.content)\n",
    "                f.write('./poster/{}.png'.format(str(count)) + '   ')\n",
    "                count += 1\n",
    "\n",
    "            \n",
    "            for tag in tags:\n",
    "                txt = tag.find_element_by_tag_name('a')\n",
    "#                 print(str(txt.text))\n",
    "                f.write(str(txt.text) + '  ')\n",
    "\n",
    "            f.write('\\n')\n",
    "            f.flush()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫爬取：http://pngimg.com/\n",
    "分两个块：第一个块是分析主页面的类别标签分布统计；第二块是进入子类页面，开始模拟右键下载图片操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import ssl\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    "browser = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FullNames = {}\n",
    "catalogs = browser.find_elements_by_class_name('catalog')\n",
    "for log in catalogs:\n",
    "    category = log.find_element_by_class_name('category')\n",
    "    ParaName = category.find_element_by_tag_name('a').get_attribute('href').split('/')[-2]\n",
    "    print('ParaNmae:',ParaName)\n",
    "    sub_category = log.find_element_by_class_name('sub_category')\n",
    "    a=sub_category.find_elements_by_tag_name('a')\n",
    "    b=sub_category.find_elements_by_tag_name('span')\n",
    "    subNames = []\n",
    "    for i,j in zip(a,b):\n",
    "        intj = int(j.text)\n",
    "        texti = i.get_attribute('href').split('/')[-2]\n",
    "        print('subName:',texti)\n",
    "        if intj >= 90:\n",
    "            subNames.append([texti,intj])\n",
    "    if len(subNames) > 0:\n",
    "        FullNames[ParaName] = subNames\n",
    "\n",
    "with open('FullPngNames.json','w') as f:\n",
    "    json.dump(FullNames, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# browser.set_page_load_timeout(100)\n",
    "with open('FullPngNames.json','r') as f:\n",
    "    dicts = json.load(f)\n",
    "for ParaName in dicts:\n",
    "    subNames = dicts[ParaName]\n",
    "    for subName in subNames:\n",
    "        print('ParaNmae:',ParaName,'subName:',subName[0])\n",
    "        sub_path = '../pngimg/{}/{}'.format(ParaName,subName[0])\n",
    "        if not os.path.isdir(sub_path):\n",
    "            os.makedirs(sub_path)\n",
    "            try:\n",
    "                browser.get('http://pngimg.com/imgs/{0}/{1}/'.format(ParaName,subName[0]))\n",
    "            finally:\n",
    "                count = 1\n",
    "                alls = browser.find_elements_by_class_name('png_png')\n",
    "                for a in alls:\n",
    "                    pic_path = a.find_element_by_tag_name('img').get_attribute('src')\n",
    "                    if pic_path is None:\n",
    "                        continue\n",
    "                    try:\n",
    "                        r = requests.get(pic_path)\n",
    "                        with open('../pngimg/{}/{}/{}.png'.format(ParaName,subName[0],subName[0]+'_'+str(count)), 'wb') as ff:\n",
    "                            ff.write(r.content)\n",
    "                            count += 1\n",
    "                            print('it is processing the %dth' % count)\n",
    "                    except:\n",
    "                        continue\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DelMask(path):\n",
    "    img = cv2.imread(path,-1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)\n",
    "    h, w, c = img.shape\n",
    "    if max(h,w) / min(h,w) > 4:\n",
    "        return None\n",
    "    mask = img[:,:,3]\n",
    "    obj = img[:,:,:3]\n",
    "    obj[mask == 0] = [255,255,255]\n",
    "    img = Image.fromarray(obj)\n",
    "    img.thumbnail((1024,1024))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "for root, dirs, files in os.walk('../imgPNG/ori/'):\n",
    "    for f in files:\n",
    "        path = os.path.join(root,f)\n",
    "        try:\n",
    "            img = DelMask(path)\n",
    "            if img is not None:\n",
    "                new_path = root.replace('/ori/','/1024/')\n",
    "                if not os.path.isdir(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                img.save(path.replace('/ori/','/1024/').replace('png','jpg'))\n",
    "                print('processing %d pic...' % count)\n",
    "                count += 1\n",
    "        except:\n",
    "            print(path)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取明星脸[百度百科](https://baike.baidu.com/starrank?fr=lemmaxianhua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "# sys.setdefaultencoding('utf8') # 设置编码\n",
    "url = 'http://baike.baidu.com/starrank?fr=lemmaxianhua'\n",
    "driver = webdriver.Chrome('./chromedriver') # 创建一个driver用于打开网页，记得找到brew安装的chromedriver的位置，在创建driver的时候指定这个位置\n",
    "driver.get(url) # 打开网页\n",
    "name_counter = 1\n",
    "page = 0;\n",
    "while page < 2: # 共50页，这里是手工指定的\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    current_names = soup.select('div.ranking-table') # 选择器用ranking-table css class，可以取出包含本周、上周的两个table的div标签\n",
    "    print('current_names',current_names)\n",
    "    for current_name_list in current_names:\n",
    "    # print current_name_list['data-cat']\n",
    "        if current_name_list['data-cat'] == 'thisWeek': # 这次我只想抓取本周，如果想抓上周，改一下这里为lastWeek即可\n",
    "            names = current_name_list.select('td.star-name > a') # beautifulsoup选择器语法\n",
    "            counter = 0;\n",
    "            for star_name in names:\n",
    "                counter = counter + 1;\n",
    "                print (star_name.text) # 明星的名字是a标签里面的文本，虽然a标签下面除了文本还有一个与文本同级别的img标签，但是.text输出的只是文本而已\n",
    "                name_counter = name_counter + 1;\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'下一页')]\").click() # selenium的xpath用法，找到包含“下一页”的a标签去点击\n",
    "    page = page + 1\n",
    "    time.sleep(2) # 睡2秒让网页加载完再去读它的html代码\n",
    "print (name_counter) # 共爬取得明星的名字数量\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
